{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurent Neural Network for text generation (character level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the corpus - Compilation of Shakespeare texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file length is:  1115393\n",
      "\n",
      "All existing characters in the corpus: \n",
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "Number of unique characters: 100\n"
     ]
    }
   ],
   "source": [
    "# All characters available\n",
    "all_characters = string.printable\n",
    "vocab = set(all_characters)\n",
    "n_characters = len(vocab)\n",
    "\n",
    "# give every character an index\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "file = open('./shakespeare.txt').read()\n",
    "file_len = len(file)\n",
    "print('The file length is: ', file_len)\n",
    "print()\n",
    "print(\"All existing characters in the corpus: \\n\"+str(all_characters))\n",
    "print(\"Number of unique characters: \"+str(n_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to create chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set on. This paltering\n",
      "Becomes not Rome, nor has Coriolanus\n",
      "Deserved this so dishonour'd rub, laid falsely\n",
      "I' the plain way of his merit.\n",
      "\n",
      "CORIOLANUS:\n",
      "Tell me of corn!\n",
      "This was my speech, and I will sp\n"
     ]
    }
   ],
   "source": [
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string, word_to_ix):    \n",
    "    idxs = [word_to_ix[c] for c in string]\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 88,  11,  32,  13,  94,  42])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tensor('abcDEF', word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input will be all characters up to the last, and the target will be all characters from the first. <br>\n",
    "Example: Input:\"We shall go!\" --> Target: \"e shall go!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set(chunk_len):    \n",
    "    chunk = random_chunk(chunk_len)\n",
    "    inp = char_tensor(chunk[:-1], word_to_ix)\n",
    "    target = char_tensor(chunk[1:], word_to_ix)\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 56,  63,  56,  99,  78,  77,   0,  56,  65,  60]),\n",
       " tensor([ 63,  56,  99,  78,  77,   0,  56,  65,  60,  56]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_training_set(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_hidden_layers, num_output):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(num_features, num_hidden, num_hidden_layers, \n",
    "                          batch_first=True, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.linear_out = nn.Linear(num_hidden, num_output)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        output, hidden = self.rnn(inputs, hidden)\n",
    "        seq_len, batch, input_size\n",
    "        output = self.linear_out(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.num_hidden_layers, 1, self.num_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / chunk_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
