{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurent Neural Network for MNIST digit prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 77777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107f38a30>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_hidden_layers, num_output):\n",
    "        super(RNN, self).__init__()        \n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(num_features, num_hidden, \n",
    "                          num_layers=num_hidden_layers, \n",
    "                          batch_first=True, #input/output tensors to be of shape (batch_dim, seq_dim, feature_dim)\n",
    "                          nonlinearity='relu')\n",
    "\n",
    "        self.linear_out = nn.Linear(num_hidden, num_output)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.num_hidden_layers, inputs.size(0), self.num_hidden) # zero vector hidden state            \n",
    "        output, hidden = self.rnn(inputs, h0)        \n",
    "        # Get only hidden state of last time step --> the goal is to get the final prediction to know what digit it is\n",
    "        output = self.linear_out(output[:, -1, :]) # output.size() --> 128, 10\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "\n",
    "num_features = 28\n",
    "num_hidden = 100\n",
    "num_hidden_layers = 2\n",
    "num_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(num_features, num_hidden, num_hidden_layers, num_output)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of steps to unroll\n",
    "seq_dim = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.view(-1, seq_dim, num_features))\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(images), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(images)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "            outputs = model(images.view(-1, seq_dim, num_features))\n",
    "            test_loss += loss_function(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}. Accuracy: {}'.format(test_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and evaluate on test set while it is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.017964\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.004676\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.005399\n",
      "====> Epoch: 1 Average loss: 0.0071\n",
      "====> Test set loss: 0.0047. Accuracy: 81\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.004119\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.003240\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.003839\n",
      "====> Epoch: 2 Average loss: 0.0040\n",
      "====> Test set loss: 0.0044. Accuracy: 82\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.003488\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.003391\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.003729\n",
      "====> Epoch: 3 Average loss: 0.0037\n",
      "====> Test set loss: 0.0031. Accuracy: 88\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect one image to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is: 3\n",
      "The real value is: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    pass\n",
    "\n",
    "img = images.data.numpy()[2]\n",
    "plt.imshow(img.reshape((28, 28)))\n",
    "out = model(images[2].view(-1, seq_dim, num_features))\n",
    "__, pred = torch.max(out.data, 1)\n",
    "\n",
    "print(\"The prediction is: \"+str(pred.item()))\n",
    "print(\"The real value is: \"+str(labels.data.numpy()[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
